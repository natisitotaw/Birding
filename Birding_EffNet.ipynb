{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5468571,"sourceType":"datasetVersion","datasetId":534640}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-10T07:26:34.794669Z","iopub.execute_input":"2024-05-10T07:26:34.795334Z","iopub.status.idle":"2024-05-10T07:26:40.603000Z","shell.execute_reply.started":"2024-05-10T07:26:34.795302Z","shell.execute_reply":"2024-05-10T07:26:40.602215Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:24:39.349710Z","iopub.execute_input":"2024-05-10T08:24:39.350327Z","iopub.status.idle":"2024-05-10T08:24:39.364430Z","shell.execute_reply.started":"2024-05-10T08:24:39.350295Z","shell.execute_reply":"2024-05-10T08:24:39.363423Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import pathlib \nimport numpy as np\n\ndata_dir = pathlib.Path(\"/kaggle/input/100-bird-species/train\")\nclass_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # creating a list of class names from subdirectory \nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:31:05.501360Z","iopub.execute_input":"2024-05-10T07:31:05.501731Z","iopub.status.idle":"2024-05-10T07:31:05.512306Z","shell.execute_reply.started":"2024-05-10T07:31:05.501702Z","shell.execute_reply":"2024-05-10T07:31:05.511421Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"525"},"metadata":{}}]},{"cell_type":"code","source":"# Define device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:26:40.605011Z","iopub.execute_input":"2024-05-10T07:26:40.605389Z","iopub.status.idle":"2024-05-10T07:26:40.627836Z","shell.execute_reply.started":"2024-05-10T07:26:40.605363Z","shell.execute_reply":"2024-05-10T07:26:40.626881Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Define data paths\ntrain_path = \"/kaggle/input/100-bird-species/train\"\nval_path = \"/kaggle/input/100-bird-species/valid\"\ntest_path = \"/kaggle/input/100-bird-species/test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:26:40.629087Z","iopub.execute_input":"2024-05-10T07:26:40.629358Z","iopub.status.idle":"2024-05-10T07:26:40.644405Z","shell.execute_reply.started":"2024-05-10T07:26:40.629335Z","shell.execute_reply":"2024-05-10T07:26:40.643675Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define batch size\nbatch_size = 64\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:26:40.645387Z","iopub.execute_input":"2024-05-10T07:26:40.645662Z","iopub.status.idle":"2024-05-10T07:26:40.654239Z","shell.execute_reply.started":"2024-05-10T07:26:40.645636Z","shell.execute_reply":"2024-05-10T07:26:40.653460Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=transform)\nval_dataset = torchvision.datasets.ImageFolder(root=val_path, transform=transform)\ntest_dataset = torchvision.datasets.ImageFolder(root=test_path, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:26:40.656887Z","iopub.execute_input":"2024-05-10T07:26:40.657628Z","iopub.status.idle":"2024-05-10T07:27:27.933952Z","shell.execute_reply.started":"2024-05-10T07:26:40.657597Z","shell.execute_reply":"2024-05-10T07:27:27.932934Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:27:27.935215Z","iopub.execute_input":"2024-05-10T07:27:27.935568Z","iopub.status.idle":"2024-05-10T07:27:27.941251Z","shell.execute_reply.started":"2024-05-10T07:27:27.935538Z","shell.execute_reply":"2024-05-10T07:27:27.940240Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:29:41.775306Z","iopub.execute_input":"2024-05-10T07:29:41.775888Z","iopub.status.idle":"2024-05-10T07:29:41.780228Z","shell.execute_reply.started":"2024-05-10T07:29:41.775859Z","shell.execute_reply":"2024-05-10T07:29:41.779170Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load EfficientNet_B0 model with pretrained weights\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:30:10.801046Z","iopub.execute_input":"2024-05-10T07:30:10.801690Z","iopub.status.idle":"2024-05-10T07:30:10.999211Z","shell.execute_reply.started":"2024-05-10T07:30:10.801656Z","shell.execute_reply":"2024-05-10T07:30:10.998208Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Freeze the pretrained parameters\nfor param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:27:28.604128Z","iopub.execute_input":"2024-05-10T07:27:28.604431Z","iopub.status.idle":"2024-05-10T07:27:28.610272Z","shell.execute_reply.started":"2024-05-10T07:27:28.604406Z","shell.execute_reply":"2024-05-10T07:27:28.609346Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:31:14.379061Z","iopub.execute_input":"2024-05-10T07:31:14.379742Z","iopub.status.idle":"2024-05-10T07:31:14.391639Z","shell.execute_reply.started":"2024-05-10T07:31:14.379708Z","shell.execute_reply":"2024-05-10T07:31:14.390582Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model,\n        input_size=(64, 3, 224, 224), # (batch_size, color_channels, height, width)\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:24:44.878877Z","iopub.execute_input":"2024-05-10T08:24:44.879505Z","iopub.status.idle":"2024-05-10T08:24:45.090259Z","shell.execute_reply.started":"2024-05-10T08:24:44.879474Z","shell.execute_reply":"2024-05-10T08:24:45.089382Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [64, 3, 224, 224]    [64, 525]            --                   True\n├─Sequential (features)                                      [64, 3, 224, 224]    [64, 1280, 7, 7]     --                   True\n│    └─Conv2dNormActivation (0)                              [64, 3, 224, 224]    [64, 32, 112, 112]   --                   True\n│    │    └─Conv2d (0)                                       [64, 3, 224, 224]    [64, 32, 112, 112]   864                  True\n│    │    └─BatchNorm2d (1)                                  [64, 32, 112, 112]   [64, 32, 112, 112]   64                   True\n│    │    └─SiLU (2)                                         [64, 32, 112, 112]   [64, 32, 112, 112]   --                   --\n│    └─Sequential (1)                                        [64, 32, 112, 112]   [64, 16, 112, 112]   --                   True\n│    │    └─MBConv (0)                                       [64, 32, 112, 112]   [64, 16, 112, 112]   1,448                True\n│    └─Sequential (2)                                        [64, 16, 112, 112]   [64, 24, 56, 56]     --                   True\n│    │    └─MBConv (0)                                       [64, 16, 112, 112]   [64, 24, 56, 56]     6,004                True\n│    │    └─MBConv (1)                                       [64, 24, 56, 56]     [64, 24, 56, 56]     10,710               True\n│    └─Sequential (3)                                        [64, 24, 56, 56]     [64, 40, 28, 28]     --                   True\n│    │    └─MBConv (0)                                       [64, 24, 56, 56]     [64, 40, 28, 28]     15,350               True\n│    │    └─MBConv (1)                                       [64, 40, 28, 28]     [64, 40, 28, 28]     31,290               True\n│    └─Sequential (4)                                        [64, 40, 28, 28]     [64, 80, 14, 14]     --                   True\n│    │    └─MBConv (0)                                       [64, 40, 28, 28]     [64, 80, 14, 14]     37,130               True\n│    │    └─MBConv (1)                                       [64, 80, 14, 14]     [64, 80, 14, 14]     102,900              True\n│    │    └─MBConv (2)                                       [64, 80, 14, 14]     [64, 80, 14, 14]     102,900              True\n│    └─Sequential (5)                                        [64, 80, 14, 14]     [64, 112, 14, 14]    --                   True\n│    │    └─MBConv (0)                                       [64, 80, 14, 14]     [64, 112, 14, 14]    126,004              True\n│    │    └─MBConv (1)                                       [64, 112, 14, 14]    [64, 112, 14, 14]    208,572              True\n│    │    └─MBConv (2)                                       [64, 112, 14, 14]    [64, 112, 14, 14]    208,572              True\n│    └─Sequential (6)                                        [64, 112, 14, 14]    [64, 192, 7, 7]      --                   True\n│    │    └─MBConv (0)                                       [64, 112, 14, 14]    [64, 192, 7, 7]      262,492              True\n│    │    └─MBConv (1)                                       [64, 192, 7, 7]      [64, 192, 7, 7]      587,952              True\n│    │    └─MBConv (2)                                       [64, 192, 7, 7]      [64, 192, 7, 7]      587,952              True\n│    │    └─MBConv (3)                                       [64, 192, 7, 7]      [64, 192, 7, 7]      587,952              True\n│    └─Sequential (7)                                        [64, 192, 7, 7]      [64, 320, 7, 7]      --                   True\n│    │    └─MBConv (0)                                       [64, 192, 7, 7]      [64, 320, 7, 7]      717,232              True\n│    └─Conv2dNormActivation (8)                              [64, 320, 7, 7]      [64, 1280, 7, 7]     --                   True\n│    │    └─Conv2d (0)                                       [64, 320, 7, 7]      [64, 1280, 7, 7]     409,600              True\n│    │    └─BatchNorm2d (1)                                  [64, 1280, 7, 7]     [64, 1280, 7, 7]     2,560                True\n│    │    └─SiLU (2)                                         [64, 1280, 7, 7]     [64, 1280, 7, 7]     --                   --\n├─AdaptiveAvgPool2d (avgpool)                                [64, 1280, 7, 7]     [64, 1280, 1, 1]     --                   --\n├─Sequential (classifier)                                    [64, 1280]           [64, 525]            --                   True\n│    └─Dropout (0)                                           [64, 1280]           [64, 1280]           --                   --\n│    └─Linear (1)                                            [64, 1280]           [64, 525]            672,525              True\n============================================================================================================================================\nTotal params: 4,680,073\nTrainable params: 4,680,073\nNon-trainable params: 0\nTotal mult-adds (G): 24.66\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 6904.45\nParams size (MB): 18.72\nEstimated Total Size (MB): 6961.71\n============================================================================================================================================"},"metadata":{}}]},{"cell_type":"code","source":"\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:31:17.443074Z","iopub.execute_input":"2024-05-10T07:31:17.443797Z","iopub.status.idle":"2024-05-10T07:31:17.450155Z","shell.execute_reply.started":"2024-05-10T07:31:17.443763Z","shell.execute_reply":"2024-05-10T07:31:17.449109Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    print('-' * 10)\n\n    # Set model to training mode\n    model.train()\n\n    running_loss = 0.0\n    correct_predictions = 0\n\n    # Training phase\n    for images, labels in tqdm(train_loader, desc=f'Training - Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct_predictions += torch.sum(preds == labels.data)\n\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_accuracy = correct_predictions.double() / len(train_dataset)\n    print(f'Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    val_correct_predictions = 0\n\n    with torch.no_grad():\n        for val_images, val_labels in tqdm(val_loader, desc=f'Validation - Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n            val_images = val_images.to(device)\n            val_labels = val_labels.to(device)\n\n            val_outputs = model(val_images)\n            val_loss = criterion(val_outputs, val_labels)\n\n            val_running_loss += val_loss.item() * val_images.size(0)\n            _, val_preds = torch.max(val_outputs, 1)\n            val_correct_predictions += torch.sum(val_preds == val_labels.data)\n\n    val_epoch_loss = val_running_loss / len(val_dataset)\n    val_epoch_accuracy = val_correct_predictions.double() / len(val_dataset)\n    print(f'Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_accuracy:.4f}')\n\nprint('Training complete')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T07:31:21.188078Z","iopub.execute_input":"2024-05-10T07:31:21.188696Z","iopub.status.idle":"2024-05-10T08:18:31.873194Z","shell.execute_reply.started":"2024-05-10T07:31:21.188666Z","shell.execute_reply":"2024-05-10T08:18:31.872166Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 1/5: 100%|██████████| 1323/1323 [14:51<00:00,  1.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 1.0149, Accuracy: 0.7765\n","output_type":"stream"},{"name":"stderr","text":"Validation - Epoch 1/5: 100%|██████████| 42/42 [00:21<00:00,  1.92batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2464, Accuracy: 0.9364\nEpoch 2/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 2/5: 100%|██████████| 1323/1323 [07:50<00:00,  2.81batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3040, Accuracy: 0.9183\n","output_type":"stream"},{"name":"stderr","text":"Validation - Epoch 2/5: 100%|██████████| 42/42 [00:07<00:00,  5.53batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1914, Accuracy: 0.9490\nEpoch 3/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 3/5: 100%|██████████| 1323/1323 [07:50<00:00,  2.81batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.2186, Accuracy: 0.9390\n","output_type":"stream"},{"name":"stderr","text":"Validation - Epoch 3/5: 100%|██████████| 42/42 [00:07<00:00,  5.50batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1775, Accuracy: 0.9459\nEpoch 4/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 4/5: 100%|██████████| 1323/1323 [07:50<00:00,  2.81batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1715, Accuracy: 0.9502\n","output_type":"stream"},{"name":"stderr","text":"Validation - Epoch 4/5: 100%|██████████| 42/42 [00:07<00:00,  5.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1977, Accuracy: 0.9444\nEpoch 5/5\n----------\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 5/5: 100%|██████████| 1323/1323 [07:53<00:00,  2.80batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1437, Accuracy: 0.9575\n","output_type":"stream"},{"name":"stderr","text":"Validation - Epoch 5/5: 100%|██████████| 42/42 [00:09<00:00,  4.65batch/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1519, Accuracy: 0.9623\nTraining complete\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test phase\nmodel.eval()\ntest_correct_predictions = 0\n\nwith torch.no_grad():\n    for test_images, test_labels in tqdm(test_loader, desc='Testing', unit='batch'):\n        test_images = test_images.to(device)\n        test_labels = test_labels.to(device)\n\n        test_outputs = model(test_images)\n        _, test_preds = torch.max(test_outputs, 1)\n\n        test_correct_predictions += torch.sum(test_preds == test_labels.data)\n\ntest_accuracy = test_correct_predictions.double() / len(test_dataset)\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:18:59.380993Z","iopub.execute_input":"2024-05-10T08:18:59.381621Z","iopub.status.idle":"2024-05-10T08:19:24.659514Z","shell.execute_reply.started":"2024-05-10T08:18:59.381593Z","shell.execute_reply":"2024-05-10T08:19:24.658652Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 42/42 [00:25<00:00,  1.66batch/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.9710\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\n\n# Function to load image from URL or local file path\ndef load_image(image_path):\n    if image_path.startswith('http'):\n        response = requests.get(image_path)\n        image = Image.open(BytesIO(response.content))\n    else:\n        image = Image.open(image_path)\n    return image\n\n# URL or local file path of the image you want to test\nimage_path = \"https://th.bing.com/th/id/R.3678f60ff36f2673189180b788e7485b?rik=XKeHnRUjLcNI%2bg&pid=ImgRaw&r=0\"  # Update with your image URL or file path\n\n# Load the image\nimage = load_image(image_path)\n\n# Preprocess the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\npreprocessed_image = transform(image).unsqueeze(0)  # Add batch dimension\n\n# Move preprocessed image to the appropriate device\npreprocessed_image = preprocessed_image.to(device)\n\n# Pass the preprocessed image through the model\nwith torch.no_grad():\n    model.eval()  # Set model to evaluation mode\n    outputs = model(preprocessed_image)\n\n# Get the predicted class\n_, predicted = torch.max(outputs, 1)\npredicted_class = class_names[predicted.item()]\n\nprint(\"Predicted class:\", predicted_class)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:29:42.548365Z","iopub.execute_input":"2024-05-10T08:29:42.548714Z","iopub.status.idle":"2024-05-10T08:29:42.944390Z","shell.execute_reply.started":"2024-05-10T08:29:42.548687Z","shell.execute_reply":"2024-05-10T08:29:42.943366Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Predicted class: ASIAN GREEN BEE EATER\n","output_type":"stream"}]},{"cell_type":"code","source":"from pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n    # Create target directory\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True,\n                          exist_ok=True)\n\n    # Create model save path\n    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n    model_save_path = target_dir_path / model_name\n\n    # Save the model state_dict()\n    print(f\"[INFO] Saving model to: {model_save_path}\")\n    torch.save(obj=model.state_dict(),\n               f=model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:21:50.599526Z","iopub.execute_input":"2024-05-10T08:21:50.599911Z","iopub.status.idle":"2024-05-10T08:21:50.606045Z","shell.execute_reply.started":"2024-05-10T08:21:50.599882Z","shell.execute_reply":"2024-05-10T08:21:50.605164Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"save_model(model,'/kaggle/working/','birdingEff.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:22:07.169448Z","iopub.execute_input":"2024-05-10T08:22:07.170091Z","iopub.status.idle":"2024-05-10T08:22:07.237212Z","shell.execute_reply.started":"2024-05-10T08:22:07.170057Z","shell.execute_reply":"2024-05-10T08:22:07.236277Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[INFO] Saving model to: /kaggle/working/birdingEff.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate model size\nmodel_size = Path('/kaggle/working/birdingEff.pth').stat().st_size / (1024*1024)  # in MB\nprint(f\"Model size: {model_size:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:22:22.106669Z","iopub.execute_input":"2024-05-10T08:22:22.107039Z","iopub.status.idle":"2024-05-10T08:22:22.112656Z","shell.execute_reply.started":"2024-05-10T08:22:22.107002Z","shell.execute_reply":"2024-05-10T08:22:22.111627Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model size: 18.14 MB\n","output_type":"stream"}]}]}